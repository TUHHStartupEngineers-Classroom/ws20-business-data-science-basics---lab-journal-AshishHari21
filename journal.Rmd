---
title: "Journal (reproducible report)"
author: "Ashish Jayaram"
date: "2020-11-05"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

This is a solution website for 4 challenges given in the course "Business Data Science Basics".Below you can see both R-scripts and results of individual challenges.All the R scripts are also accumulated in git hub repository inside folder named "solution"

# Challenge 1 : bike sales report

Last compiled: `r Sys.Date()`

In the first task, Different analysis was carried out on bike sales with respect to year and location.
First analysis was on sales by location and it was found out to be North Rhine-Westphalia with highest sales
Second analysis was on sales by years for different states. 

## R script

```{r}
# Data Science at TUHH ------------------------------------------------------
# SALES ANALYSIS ----

# 1.0 Load libraries ----
library(tidyverse)
library(readxl)
library(lubridate)
library("writexl")

# 2.0 Importing Files ----
bikes_tbl      <- read_excel(path = "~/Data science/DS_101/00_data/01_bike_sales/01_raw_data/bikes.xlsx")
orderlines_tbl <- read_excel(path = "~/Data science/DS_101/00_data/01_bike_sales/01_raw_data/orderlines.xlsx")
bikeshops_tbl <- read_excel(path = "~/Data science/DS_101/00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")

# 3.0 Examining Data ----
View(orderlines_tbl)
View(bikeshops_tbl)
View(bikes_tbl)

# 4.0 Joining Data ----
bike_orderlines_joined_tbl <- orderlines_tbl %>% left_join(bikes_tbl, by = c("product.id" = "bike.id")) %>% left_join(bikeshops_tbl, by = c("customer.id" = "bikeshop.id"))

# 5.0 Wrangling Data ----
bike_orderlines_wrangled_tbl <- bike_orderlines_joined_tbl %>% separate(col    = location, into   = c("city", "State"), sep    = ",") %>% 
mutate(total.price = price * quantity) %>% rename(bikeshop = name) %>% set_names(names(.) %>% str_replace_all("\\.", "_"))

# 6.0 Business Insights ----

# 6.1 Sales by State ----

# Step 1 - Manipulate
sales_by_state_tbl <- bike_orderlines_wrangled_tbl %>% select(State, total_price) %>% group_by(State) %>% summarize(sales = sum(total_price)) %>%
mutate(sales_text = scales::dollar(sales, big.mark = ".", decimal.mark = ",", prefix = "", suffix = " €"))
View(sales_by_state_tbl)

sales_by_city_tbl <- bike_orderlines_wrangled_tbl %>% select(city, total_price) %>% group_by(city) %>% summarize(sales = sum(total_price)) %>%
mutate(sales_text = scales::dollar(sales, big.mark = ".", decimal.mark = ",", prefix = "", suffix = " €"))
View(sales_by_city_tbl)

# Step 2 - Visualize


sales_by_state_tbl %>%
ggplot(aes(x = State, y = sales)) +
geom_col(fill = "#2DC6D6") + 
geom_label(aes(label = sales_text)) +
geom_smooth(method = "lm", se = FALSE) + 
scale_y_continuous(labels = scales::dollar_format(big.mark = ".", decimal.mark = ",", prefix = "", suffix = " €")) +
labs(title    = "Revenue by state", subtitle = "Upward Trend", x = "", y = "Revenue")+
theme(axis.text.x = element_text(angle = 45, hjust = 1))


sales_by_city_tbl %>%
ggplot(aes(x = city, y = sales)) +
geom_col(fill = "#2DC6D6") + 
geom_label(aes(label = sales_text)) +
geom_smooth(method = "lm", se = FALSE) + 
scale_y_continuous(labels = scales::dollar_format(big.mark = ".", decimal.mark = ",", prefix = "", suffix = " €")) +
labs(title    = "Revenue by city", subtitle = "Upward Trend", x = "", y = "Revenue")+
theme(axis.text.x = element_text(angle = 45, hjust = 1))


# 6.2 Sales by Year and location ----

# Step 1 - Manipulate
sales_by_location_year_tbl <- bike_orderlines_wrangled_tbl %>% select(State, order_date, total_price) %>%  mutate(year = year(order_date)) %>%
group_by(State ,year) %>% summarize(sales = sum(total_price)) %>% mutate(sales_text = scales::dollar(sales, big.mark = ".", decimal.mark = ",", prefix = "", suffix = " €"))
View(sales_by_location_year_tbl)

# Step 2 - Visualize
sales_by_location_year_tbl %>%
ggplot(aes(x = year, y = sales)) +
geom_col(fill = "#2DC6D6") + 
geom_label(aes(label = sales_text)) +
geom_smooth(method = "lm", se = FALSE) + 
scale_y_continuous(labels = scales::dollar_format(big.mark = ".", decimal.mark = ",", prefix = "", suffix = " €")) +
labs(title    = "Revenue by state and year", subtitle = "Upward Trend", x = "", y = "Revenue")+
theme(axis.text.x = element_text(angle = 45, hjust = 1))+
facet_wrap(vars(State))


# 7.0 Writing Files ----

# 7.1 Excel ----
bike_orderlines_wrangled_tbl %>% write_xlsx("~/Data science/DS_101/00_data/01_bike_sales/02_wrangled_data/bike_orderlines.xlsx")
# 7.2 CSV ----
bike_orderlines_wrangled_tbl %>% write_csv("~/Data science/DS_101/00_data/01_bike_sales/02_wrangled_data/bike_orderlines.csv")
# 7.3 RDS ----
bike_orderlines_wrangled_tbl %>% write_rds("~/Data science/DS_101/00_data/01_bike_sales/02_wrangled_data/bike_orderlines.rds")
```

# Data Acquisition
## Challenge 1. Requesting data from API
This piece of code is used to request weather data from provider named "7timer"
```{r}
library(httr)
resp <- GET("http://www.7timer.info/bin/api.pl?lon=113.17&lat=23.09&product=astro&output=json")
resp
```
## Challenge 2. web scraping
Following piece of code gathers the cycle models from radon bikes and its priceses
```{r}
library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi) 

url_home <- "https://www.radon-bikes.de/"
xopen(url_home) 

html_home <- read_html(url_home)
list_of_product_types <- html_home%>%
  html_nodes(css = ".megamenu__item > a")%>%
  html_text()


list_of_products_url <- html_home %>%
  html_nodes(".megamenu__item > a") %>%
  html_attr("href") %>%
  enframe(name = NULL, value = "url") %>%
  mutate(url = str_glue("https://www.radon-bikes.de{url}"))

# selecting first bike category url
bike_category_url <- list_of_products_url$url[1]

xopen(bike_category_url)

# Get the URLs for the bikes of the first category
html_bike_category  <- read_html(bike_category_url)
temp_url <- html_bike_category%>%
  html_node(".a-button--hollow-secondary")%>%
  html_attr("href")%>%
  enframe(name = NULL, value = "url") %>%
  mutate(url = str_glue("https://www.radon-bikes.de{url}"))

temp_url <- temp_url$url[1]
xopen(temp_url)

bike_category_grid_html <- read_html(temp_url)

list_of_product_names <- bike_category_grid_html%>%
  html_nodes(css=".m-bikegrid__info > a > div > h4")%>%
  html_text%>%
  stringr::str_replace_all(pattern = "\n","")%>%
  stringr::str_replace_all(pattern = "  ","")%>%
  enframe(name = NULL, value = "NAME")



list_of_product_prices <- bike_category_grid_html%>%
  html_nodes(css=".m-bikegrid__price--active")%>%
  html_text()%>%
  stringr::str_extract(pattern = "[0-9€]+")%>%
  stringr::str_replace(pattern = "€","")%>%
  as.numeric()%>%
  enframe(name = NULL, value = "PRICE")

list_of_product_prices = na.omit(list_of_product_prices)

bike_df <- data.frame(list_of_product_names,list_of_product_prices)
saveRDS(bike_df, "bike_data.rds")
head(bike_df,10)
```
# Challenge 3: Data Wrangling
This challenge had lot of challenges, and hence was not able to complete it fully. At first I was not able to allocate a vector for merged data since the data was too huge.And when converting patant_tbl to data table system always used to crash. Hence I have answered only the 1st question in this challenge. Since 2nd challenge and third challenge requires data to be merged I was not able to complete it.
```{r}
# library(tidyverse)
# library(dplyr)
# library(vroom)
# library(tictoc)
# library(data.table)

# col_types <- cols_only(
  # id = col_character(),
  # type = col_character(),
  # number = col_character(),
  # country = col_character(),
  # date = col_date("%d-%m-%y"),
  # abstract = col_character(),
  # title = col_character(),
  # kind = col_character(),
  # num_claims = col_double(),
  # filename = col_character(),
  # withdrawn = col_double()
# )

# patent_tbl <- vroom(
  # file       = "~/Data science/DS_101/00_data/patent/patent.tsv", 
  # delim      = "\t", 
  # col_types  = col_types,
  # na         = c("", "NA", "NULL")
# )
# col_types_PA <- cols_only(
  # patent_id = col_character()
  # assignee_id = col_character(),
  # location_id = col_character()
  # )
# patent_assignee_tbl <- vroom(
  # file       = "~/Data science/DS_101/00_data/patent/patent_assignee.tsv", 
  # delim      = "\t", 
  # col_types  = col_types_PA,
  # na         = c("", "NA", "NULL")
# )
 # col_types_A <- cols_only(
   # id = col_character(),
   # type = col_double(),
   # name_first = col_character(),
   # name_last = col_character(),
   # organization = col_character()
 # )
# assignee_tbl <- vroom(
  # file       = "~/Data science/DS_101/00_data/patent/assignee.tsv", 
  # delim      = "\t", 
  # col_types  = col_types_A,
  # na         = c("", "NA", "NULL")
# )

# # gives the number of times a company is repeated in decending order. From which 
# # we can get highest to lowest number of patient for each company
# assignee_tbl %>%
  # filter(!is.na(organization)) %>%
  # count(organization) %>%
  # arrange(desc(n))
```
# challenge 4: Data Visualization
## Cumilative Covid cases

```{r}
library(tidyverse)
library(data.table)
library(ggplot2)
library(ggrepel)
url <- "https://opendata.ecdc.europa.eu/covid19/casedistribution/csv"
covid_data_tbl <- fread(url)

#check the unique country present.
covid_data_tbl$countriesAndTerritories %>% unique()

class(covid_data_tbl)
colnames(covid_data_tbl)
str(covid_data_tbl)

#getting month name column
covid_data_tbl$month_name<-months(as.Date(covid_data_tbl$dateRep))

##rolling up data to month year country Level
covid_mon_yr_country_lvl <- covid_data_tbl %>% 
  dplyr::group_by(month,month_name,year,countriesAndTerritories,geoId,countryterritoryCode,continentExp) %>% 
  dplyr::summarise(cases = sum(cases, na.rm = T)) %>% 
  dplyr::ungroup()

##creating Cummulative Cases column
covid_mon_yr_country_lvl <- covid_mon_yr_country_lvl %>% 
  dplyr::arrange(countriesAndTerritories,year,month) %>% 
  dplyr::group_by(countriesAndTerritories) %>% 
  dplyr::mutate(cumulative_cases = cumsum(cases)) %>% 
  dplyr::ungroup()

##I am filtering only for those shown in the graph and for the year = 2020
covid_mon_yr_country_lvl_fil<- covid_mon_yr_country_lvl %>% 
  dplyr::filter(countriesAndTerritories %in% c("Germany","Spain","France","United_Kingdom","United_States_of_America")& year == 2020) %>%
  dplyr::rename('Continent_Country' = countriesAndTerritories)

#Graph using ggploat
covid_mon_yr_country_lvl_fil %>% 
  mutate(label = if_else(month_name == "December",as.character(cumulative_cases),NA_character_)) %>% 
  ggplot(aes(x=month,y =cumulative_cases))+
  geom_line(aes(color = Continent_Country))+
  scale_colour_brewer(palette = "Set1")+
  scale_x_continuous(breaks=covid_mon_yr_country_lvl_fil$month,labels = covid_mon_yr_country_lvl_fil$month_name)+
  scale_y_continuous(labels = scales::dollar_format(scale = 1/1e6,
                                                    prefix = "",
                                                    suffix = "M"))+
  labs(title = "COVID-19 confirmed cases worldwide",
       subtitle =  "As of 12/5/2020,USA has the highest cases.",
       x = "Year 2020",
       y= "Cumulative Cases"
  )+
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle=45,hjust = 1))+
  geom_label_repel(aes(label=label),
                   nudge_x = 1,na.rm = TRUE)

covid_data_tbl %>% dplyr::filter(continentExp == "Europe")
sum(covid_data_tbl$cases,na.rm = T)


```
## Mortality rate

```{r}
library(tidyverse)
library(data.table)
library(ggplot2)
library(ggrepel)
library(maps)
library(ggthemes)
library(mapproj)


url <- "https://opendata.ecdc.europa.eu/covid19/casedistribution/csv"
covid_data_tbl <- fread(url)

world <- map_data("world")
colnames((world))

covid_data_tbl$countriesAndTerritories <- str_replace_all(covid_data_tbl$countriesAndTerritories,"_"," ")
world_data<- covid_data_tbl %>% 
  dplyr::mutate(countriesAndTerritories = case_when(
    countriesAndTerritories == "United Kingdom" ~ "UK",
    countriesAndTerritories == "United States of America" ~ "USA",
    countriesAndTerritories == "Czechia" ~ "Czech Republic",
    TRUE ~ countriesAndTerritories
  ))

options(scipen = 999)

country_mortality_rate<- world_data %>% 
  dplyr::group_by(countriesAndTerritories) %>% 
  dplyr::summarise(deaths = sum(deaths, na.rm = T),
                   popData2019 = nth(popData2019,1)) %>% 
  dplyr::mutate(mortality_rate = round((deaths/popData2019)*100,3))

plot_data <- country_mortality_rate %>%  
  dplyr::right_join(world,by=c("countriesAndTerritories"="region"))

plot_data %>% ggplot()+
  geom_map(map = world,aes(map_id = countriesAndTerritories,fill=mortality_rate),color = "#7f7f7f",size=0.25)+
  scale_fill_gradient(low="#FF3333",high = "#330000",name="Mortality Rate")+
  expand_limits(x= world$long,y=world$lat)+
  labs(x="",y="",title="Cnfirmed COVID-19 deaths relative to the size of population")

```